{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenducminh2206/NLP-Projects/blob/main/NER_CMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lpq_N8dpbtd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wn1EYDQLMVZ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RCfFq2ZK6Erv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/train_processed.csv')\n",
        "val_data = pd.read_csv('/content/dev_processed.csv')"
      ],
      "metadata": {
        "id": "bwiHe0pR5EQX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guCkapFCdixB"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qDaGogykgJBw"
      },
      "outputs": [],
      "source": [
        "unique_labels_train = set(label for labels in train_data['labels'].apply(eval) for label in labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IsDBeSB_gXFY"
      },
      "outputs": [],
      "source": [
        "unique_labels_val = set(label for labels in val_data['labels'].apply(eval) for label in labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d7pR5cowgjMR"
      },
      "outputs": [],
      "source": [
        "all_unique_labels = unique_labels_train.union(unique_labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA8z71Vzgq3u",
        "outputId": "92464123-aad2-42f3-da09-a5de07aa8496"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOCATION',\n",
              " 'B-MISCELLANEOUS',\n",
              " 'B-ORGANIZATION',\n",
              " 'B-PERSON',\n",
              " 'I-LOCATION',\n",
              " 'I-MISCELLANEOUS',\n",
              " 'I-ORGANIZATION',\n",
              " 'I-PERSON',\n",
              " 'O'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "all_unique_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VS8qxGXHcYvA"
      },
      "outputs": [],
      "source": [
        "label_to_id = {label: idx for idx, label in enumerate(sorted(all_unique_labels))}\n",
        "id_to_label = {id: label for label, id in label_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06PvKCd_TDrC",
        "outputId": "f309acd0-89a2-4777-d93d-3ffc76d6bfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'B-LOCATION',\n",
              " 1: 'B-MISCELLANEOUS',\n",
              " 2: 'B-ORGANIZATION',\n",
              " 3: 'B-PERSON',\n",
              " 4: 'I-LOCATION',\n",
              " 5: 'I-MISCELLANEOUS',\n",
              " 6: 'I-ORGANIZATION',\n",
              " 7: 'I-PERSON',\n",
              " 8: 'O'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEzVbuMzhEaZ",
        "outputId": "a5b8c5f2-db27-4195-b0f3-f16b8ec70fd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOCATION': 0,\n",
              " 'B-MISCELLANEOUS': 1,\n",
              " 'B-ORGANIZATION': 2,\n",
              " 'B-PERSON': 3,\n",
              " 'I-LOCATION': 4,\n",
              " 'I-MISCELLANEOUS': 5,\n",
              " 'I-ORGANIZATION': 6,\n",
              " 'I-PERSON': 7,\n",
              " 'O': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ugcto1U2djM3"
      },
      "outputs": [],
      "source": [
        "def encode_examples(texts, token_labels, tokenizer, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for i, (text, label) in enumerate(zip(texts, token_labels)):\n",
        "        # Tokenize input and get the corresponding labels\n",
        "        tokenized_inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, is_split_into_words=True)\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=0)  # Map tokens to their corresponding word ids.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        # Align label ids with word ids\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label_to_id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100)  # Pad tokens\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        input_ids.append(tokenized_inputs['input_ids'])\n",
        "        attention_masks.append(tokenized_inputs['attention_mask'])\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Prepare the training and validation data\n",
        "train_texts = train_data['tokens'].apply(eval).tolist()\n",
        "train_labels = train_data['labels'].apply(eval).tolist()\n",
        "val_texts = val_data['tokens'].apply(eval).tolist()\n",
        "val_labels = val_data['labels'].apply(eval).tolist()\n",
        "\n",
        "# Encode the training and validation datasets\n",
        "train_input_ids, train_attention_masks, train_labels = encode_examples(train_texts, train_labels, tokenizer)\n",
        "val_input_ids, val_attention_masks, val_labels = encode_examples(val_texts, val_labels, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AfVebAXQhSEA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VhcT6jHhhhwY"
      },
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "  def __init__(self, input_ids, attention_masks, labels):\n",
        "    self.input_ids = input_ids\n",
        "    self.attention_masks = attention_masks\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "        'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
        "        'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gcm2kcaah2fj"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = NERDataset(val_input_ids, val_attention_masks, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqw3Gcnh6L_"
      },
      "outputs": [],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_to_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNHWqBRLh9cA"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rwyLZrjDiZYd",
        "outputId": "5f2c55fd-e5f1-4c22-9774-2fa3d1bd3dc2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5860' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5860/5860 46:25, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.045900</td>\n",
              "      <td>0.064193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.039300</td>\n",
              "      <td>0.051711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.041200</td>\n",
              "      <td>0.051264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.014800</td>\n",
              "      <td>0.056554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.060078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.007900</td>\n",
              "      <td>0.063061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.066944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.066513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.072648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.073804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5860, training_loss=0.03119340448126947, metrics={'train_runtime': 2787.5208, 'train_samples_per_second': 33.593, 'train_steps_per_second': 2.102, 'total_flos': 6117344196433920.0, 'train_loss': 0.03119340448126947, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "VPGndN2GRYEB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'tôi tên là Nguyễn Đức Minh, sinh năm 2004, học ở Phần Lan, sinh ra và lớn lên ở Hà Nội'"
      ],
      "metadata": {
        "id": "GqF1aQqTRo1u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'LABEL_0': 'B-LOCATION',\n",
        "    'LABEL_1': 'B-MISCELLANEOUS',\n",
        "    'LABEL_2': 'B-ORGANIZATION',\n",
        "    'LABEL_3': 'B-PERSON',\n",
        "    'LABEL_4': 'I-LOCATION',\n",
        "    'LABEL_5': 'I-MISCELLANEOUS',\n",
        "    'LABEL_6': 'I-ORGANIZATION',\n",
        "    'LABEL_7': 'I-PERSON',\n",
        "    'LABEL_8': 'O'\n",
        "}"
      ],
      "metadata": {
        "id": "FkMI4nDoVEDE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
        "results = ner(text)\n",
        "for result in results:\n",
        "  if result['entity'] in label_mapping:\n",
        "    result['entity'] = label_mapping[result['entity']]\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40EfQgTeRdqw",
        "outputId": "211fd526-6990-4dcb-c6dc-3d65baaf8197"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'O',\n",
              "  'score': 0.9999763,\n",
              "  'index': 1,\n",
              "  'word': 'tôi',\n",
              "  'start': 0,\n",
              "  'end': 3},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99997747,\n",
              "  'index': 2,\n",
              "  'word': 'tên',\n",
              "  'start': 4,\n",
              "  'end': 7},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999759,\n",
              "  'index': 3,\n",
              "  'word': 'là',\n",
              "  'start': 8,\n",
              "  'end': 10},\n",
              " {'entity': 'B-PERSON',\n",
              "  'score': 0.99972826,\n",
              "  'index': 4,\n",
              "  'word': 'Nguyễn',\n",
              "  'start': 11,\n",
              "  'end': 17},\n",
              " {'entity': 'I-PERSON',\n",
              "  'score': 0.9998455,\n",
              "  'index': 5,\n",
              "  'word': 'Đức',\n",
              "  'start': 18,\n",
              "  'end': 21},\n",
              " {'entity': 'I-PERSON',\n",
              "  'score': 0.999846,\n",
              "  'index': 6,\n",
              "  'word': 'Minh',\n",
              "  'start': 22,\n",
              "  'end': 26},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99995816,\n",
              "  'index': 7,\n",
              "  'word': ',',\n",
              "  'start': 26,\n",
              "  'end': 27},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999765,\n",
              "  'index': 8,\n",
              "  'word': 'sinh',\n",
              "  'start': 28,\n",
              "  'end': 32},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99998,\n",
              "  'index': 9,\n",
              "  'word': 'năm',\n",
              "  'start': 33,\n",
              "  'end': 36},\n",
              " {'entity': 'O',\n",
              "  'score': 0.999977,\n",
              "  'index': 10,\n",
              "  'word': '2004',\n",
              "  'start': 37,\n",
              "  'end': 41},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999759,\n",
              "  'index': 11,\n",
              "  'word': ',',\n",
              "  'start': 41,\n",
              "  'end': 42},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999795,\n",
              "  'index': 12,\n",
              "  'word': 'học',\n",
              "  'start': 43,\n",
              "  'end': 46},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999794,\n",
              "  'index': 13,\n",
              "  'word': 'ở',\n",
              "  'start': 47,\n",
              "  'end': 48},\n",
              " {'entity': 'B-LOCATION',\n",
              "  'score': 0.9997478,\n",
              "  'index': 14,\n",
              "  'word': 'Phần',\n",
              "  'start': 49,\n",
              "  'end': 53},\n",
              " {'entity': 'I-LOCATION',\n",
              "  'score': 0.9997042,\n",
              "  'index': 15,\n",
              "  'word': 'Lan',\n",
              "  'start': 54,\n",
              "  'end': 57},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99997604,\n",
              "  'index': 16,\n",
              "  'word': ',',\n",
              "  'start': 57,\n",
              "  'end': 58},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999807,\n",
              "  'index': 17,\n",
              "  'word': 'sinh',\n",
              "  'start': 59,\n",
              "  'end': 63},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99998,\n",
              "  'index': 18,\n",
              "  'word': 'ra',\n",
              "  'start': 64,\n",
              "  'end': 66},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99998116,\n",
              "  'index': 19,\n",
              "  'word': 'và',\n",
              "  'start': 67,\n",
              "  'end': 69},\n",
              " {'entity': 'O',\n",
              "  'score': 0.9999809,\n",
              "  'index': 20,\n",
              "  'word': 'lớn',\n",
              "  'start': 70,\n",
              "  'end': 73},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99998116,\n",
              "  'index': 21,\n",
              "  'word': 'lên',\n",
              "  'start': 74,\n",
              "  'end': 77},\n",
              " {'entity': 'O',\n",
              "  'score': 0.99997973,\n",
              "  'index': 22,\n",
              "  'word': 'ở',\n",
              "  'start': 78,\n",
              "  'end': 79},\n",
              " {'entity': 'B-LOCATION',\n",
              "  'score': 0.99974245,\n",
              "  'index': 23,\n",
              "  'word': 'Hà',\n",
              "  'start': 80,\n",
              "  'end': 82},\n",
              " {'entity': 'I-LOCATION',\n",
              "  'score': 0.99969995,\n",
              "  'index': 24,\n",
              "  'word': 'Nội',\n",
              "  'start': 83,\n",
              "  'end': 86}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_entity(results):\n",
        "    combined_entities = {}\n",
        "    current_entity = []\n",
        "    current_label = None\n",
        "\n",
        "    for result in results:\n",
        "        if '-B' in result['entity']:\n",
        "            if current_entity:\n",
        "                combined_entities[' '.join(current_entity)] = current_label.split('-')[1]\n",
        "                current_entity = []\n",
        "\n",
        "                current_label = result['entity']\n",
        "                current_entity.append(result['word'])\n",
        "        elif 'I-' in result['entity'] and current_label and result['entity'].split('-')[1] == current_label.split('-')[1]:\n",
        "            current_entity.append(result['word'])\n",
        "\n",
        "        else:\n",
        "            if current_entity:\n",
        "                combined_entities[' '.join(current_entity)] = current_label.split('-')[1]\n",
        "                current_entity = []\n",
        "\n",
        "            current_label = result['entity'] if 'B-' in result['entity'] else None\n",
        "            if current_label:\n",
        "                current_entity.append(result['word'])\n",
        "    if current_entity:\n",
        "        combined_entities[' '.join(current_entity)] = current_label.split('-')[1]\n",
        "\n",
        "    return combined_entities\n",
        "\n",
        "\n",
        "print(process_entity(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAr02QcoGRvp",
        "outputId": "1361407f-7cce-4511-ec94-864a59117d75"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Nguyễn Đức Minh': 'PERSON', 'Phần Lan': 'LOCATION', 'Hà Nội': 'LOCATION'}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMR/4Od8ZfJB/NfqogHfBi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}