{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIwG0caTwP+/bNMqlPYgGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenducminh2206/NLP-Projects/blob/main/NER_CMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "7lpq_N8dpbtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wn1EYDQLMVZ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/train_processed.csv')\n",
        "val_data = pd.read_csv('/content/dev_processed.csv')"
      ],
      "metadata": {
        "id": "TxjNCYG1ODdk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "guCkapFCdixB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels_train = set(label for labels in train_data['labels'].apply(eval) for label in labels)"
      ],
      "metadata": {
        "id": "qDaGogykgJBw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels_val = set(label for labels in val_data['labels'].apply(eval) for label in labels)"
      ],
      "metadata": {
        "id": "IsDBeSB_gXFY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique_labels = unique_labels_train.union(unique_labels_val)"
      ],
      "metadata": {
        "id": "d7pR5cowgjMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique_labels"
      ],
      "metadata": {
        "id": "aA8z71Vzgq3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337e503f-ba61-4f18-bbb9-d11520bf42ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOCATION',\n",
              " 'B-MISCELLANEOUS',\n",
              " 'B-ORGANIZATION',\n",
              " 'B-PERSON',\n",
              " 'I-LOCATION',\n",
              " 'I-MISCELLANEOUS',\n",
              " 'I-ORGANIZATION',\n",
              " 'I-PERSON',\n",
              " 'O'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id = {label: idx for idx, label in enumerate(sorted(all_unique_labels))}\n",
        "id_to_label = {id: label for label, id in label_to_id.items()}"
      ],
      "metadata": {
        "id": "VS8qxGXHcYvA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id"
      ],
      "metadata": {
        "id": "LEzVbuMzhEaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67bbecc-881b-4b56-ec10-24a322d19082"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOCATION': 0,\n",
              " 'B-MISCELLANEOUS': 1,\n",
              " 'B-ORGANIZATION': 2,\n",
              " 'B-PERSON': 3,\n",
              " 'I-LOCATION': 4,\n",
              " 'I-MISCELLANEOUS': 5,\n",
              " 'I-ORGANIZATION': 6,\n",
              " 'I-PERSON': 7,\n",
              " 'O': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_examples(texts, token_labels, tokenizer, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for i, (text, label) in enumerate(zip(texts, token_labels)):\n",
        "        # Tokenize input and get the corresponding labels\n",
        "        tokenized_inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, is_split_into_words=True)\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=0)  # Map tokens to their corresponding word ids.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        # Align label ids with word ids\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label_to_id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100)  # Pad tokens\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        input_ids.append(tokenized_inputs['input_ids'])\n",
        "        attention_masks.append(tokenized_inputs['attention_mask'])\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Prepare the training and validation data\n",
        "train_texts = train_data['tokens'].apply(eval).tolist()\n",
        "train_labels = train_data['labels'].apply(eval).tolist()\n",
        "val_texts = val_data['tokens'].apply(eval).tolist()\n",
        "val_labels = val_data['labels'].apply(eval).tolist()\n",
        "\n",
        "# Encode the training and validation datasets\n",
        "train_input_ids, train_attention_masks, train_labels = encode_examples(train_texts, train_labels, tokenizer)\n",
        "val_input_ids, val_attention_masks, val_labels = encode_examples(val_texts, val_labels, tokenizer)"
      ],
      "metadata": {
        "id": "Ugcto1U2djM3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "AfVebAXQhSEA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "  def __init__(self, input_ids, attention_masks, labels):\n",
        "    self.input_ids = input_ids\n",
        "    self.attention_masks = attention_masks\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "        'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
        "        'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "VhcT6jHhhhwY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NERDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = NERDataset(val_input_ids, val_attention_masks, val_labels)"
      ],
      "metadata": {
        "id": "gcm2kcaah2fj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_to_id))"
      ],
      "metadata": {
        "id": "wqqw3Gcnh6L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "gNHWqBRLh9cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75929643-e4e2-4a9f-82e8-e0d8ad50a251"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "rwyLZrjDiZYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}